# YouTrack RAG Support App

Streamlit application for technical assistance based on YouTrack tickets, indexed in a local Vector DB (Chroma) and queried through retrievalâ€‘augmented generation (RAG) using OpenAI or local Ollama LLMs.îˆ€fileciteîˆ‚turn1file0îˆ

---

## 1. Overview

This app lets you:

- Connect to a YouTrack instance via URL + Bearer token  
- Load projects and issues, and index them into a Chroma vector store  
- Configure embeddings, chunking and retrieval behavior  
- Choose an LLM provider (OpenAI or Ollama) and model  
- Ask questions in natural language and get answers grounded on similar tickets  
- Save good answers as reusable â€œplaybooksâ€ in a separate memory collection  
- Persist nonâ€‘sensitive preferences locally across sessionsîˆ€fileciteîˆ‚turn1file0îˆ

The UI is organized as a **multiâ€‘phase wizard** in the sidebar:

1. YouTrack connection  
2. Embeddings & Vector DB  
3. Retrieval configuration  
4. LLM & API keys  
5. Solutions memory  
6. Chat & Results  
7. Preferences & debugîˆ€fileciteîˆ‚turn1file0îˆ

---

## 2. Features by Phase

### 2.1 Phase 1 â€“ YouTrack Connection

- Configure **YouTrack URL** and **Bearer token** (not saved to disk).  
- On â€œConnectâ€, the app creates a `YouTrackClient` and loads the list of projects.  
- A project selectbox shows entries as `Name (ShortName)`; when you select a project, issues are automatically loaded.  
- A â€œReload issuesâ€ button lets you fetch them again manually.  
- Issues are shown in a Markdown table with:
  - Clickable **ID** linking back to YouTrack (`/issue/<ID>`)  
  - Shortened **Summary** on a single lineîˆ€fileciteîˆ‚turn1file0îˆ  

---

### 2.2 Phase 2 â€“ Embeddings & Vector DB

#### Chroma path and collections

- Configurable **Chroma path** (`persist_dir`), defaulting to:
  - `/tmp/chroma` in cloud / readâ€‘only environments  
  - `<APP_DIR>/data/chroma` in local / Docker environments  
  - or a custom path via `CHROMA_DIR` env var / Streamlit secretsîˆ€fileciteîˆ‚turn1file0îˆ
- The app lists existing Chroma collections and lets you:
  - Select an existing collection  
  - Or choose `â• Create new collectionâ€¦` and specify a name
- The selected collection name is stored as:
  - `collection_selected` / `vs_collection` in `session_state` and prefs.îˆ€fileciteîˆ‚turn1file0îˆ

#### Collection management

- **Delete collection** button:
  - Requires an explicit confirmation checkbox  
  - Deletes the Chroma collection  
  - Removes the associated `<collection>__meta.json` file  
  - Clears current issues, vector handle and related prefs  
  - Leaves you on Phase 2 after a rerunîˆ€fileciteîˆ‚turn1file0îˆ  

#### Embeddings configuration

- Embedding providers:
  - `Local (sentence-transformers)` (when available and not in cloud)  
  - `OpenAI`  
- Embedding model options:
  - Local: `all-MiniLM-L6-v2`  
  - OpenAI: `text-embedding-3-small`, `text-embedding-3-large`  
- When you switch provider, the model is reset to a suitable default.  
- The chosen provider/model are used both for **indexing** and, unless overridden by metadata, for **query**.îˆ€fileciteîˆ‚turn1file0îˆ  

#### Ticket indexing (with chunking)

- â€œIndex ticketsâ€ button indexes all currently loaded issues into the selected collection.  
- Long ticket texts are **chunked** with configurable parameters (see Phase 3):
  - Tokenâ€‘based when `tiktoken` is available, otherwise whitespaceâ€‘based  
  - Metadata per chunk:
    - `parent_id` = original ticket ID  
    - `id_readable` = ticket ID  
    - `summary`, `project`  
    - `chunk_id`, `pos` (token offset) for multiâ€‘chunk ticketsîˆ€fileciteîˆ‚turn1file0îˆ  
- The embedder input combines ID, summary and chunk text to improve semantic search.  
- After indexing:
  - A `<collection>__meta.json` file is written with `provider` and `model`  
  - The `vs_*` fields in `session_state` are updated (`vs_collection`, `vs_persist_dir`, `vs_count`)  
  - A success message with the total number of indexed chunks/documents is shownîˆ€fileciteîˆ‚turn1file0îˆ  

---

### 2.3 Phase 3 â€“ Retrieval Configuration

This phase controls how results are retrieved and aggregated from Chroma.

#### Distance threshold

- Slider `max_distance` (cosine distance), default **0.9**.  
- Both KB (tickets) and MEM (playbooks) results are filtered: only those with `distance <= max_distance` are kept.îˆ€citeîˆ‚turn1file0îˆ  

Typical usage:

- Lower values â†’ more precise, fewer but highly relevant results  
- Higher values â†’ more permissive, useful when the KB is small or noisy  

#### Chunking configuration

Controls how long tickets are split when indexing:

- `enable_chunking` (checkbox)  
- `chunk_size` (tokens), default 800  
- `chunk_overlap` (tokens), default 80  
- `chunk_min`: below this size, tickets are indexed as a single document (default 512)îˆ€citeîˆ‚turn1file0îˆ  

These settings are used in **Phase 2** during indexing via `split_into_chunks`.

#### Advanced retrieval settings

Under the â€œAdvanced settingsâ€ expander:

- `show_distances`: show distance values next to results in the UI  
- `top_k`: number of KB results retrieved from Chroma (before filtering / collapsing)  
- `collapse_duplicates`: collapse multiple chunks from the same ticket in the UI  
- `per_parent_display`: max number of results per ticket shown in the UI  
- `per_parent_prompt`: max number of chunks per ticket used in the LLM prompt  
- `stitch_max_chars`: character limit when concatenating chunks into a single context blockîˆ€citeîˆ‚turn1file0îˆ  

There is also a **â€œReset to defaultsâ€** button that restores recommended values and shows a toast.

All these settings are synced to canonical keys used by the Chat phase (`top_k`, `show_distances`, `collapse_duplicates`, `per_parent_display`, `per_parent_prompt`, `stitch_max_chars`) and are persisted in prefs.îˆ€citeîˆ‚turn1file0îˆ  

---

### 2.4 Phase 4 â€“ LLM & API Keys

- LLM providers:
  - **OpenAI**  
  - **Ollama (local)** â€“ shown only if detected via HTTP `/api/tags` or `ollama list`îˆ€citeîˆ‚turn1file0îˆ  
- Provider change resets the model to:
  - `gpt-4o` for OpenAI  
  - `llama3.2` for Ollama (default)  
- Model is editable via a text input (`llm_model`).  
- Temperature slider between 0.0 and 1.5.  

**API Keys**

- The app determines whether an OpenAI key is needed based on:
  - Embeddings provider  
  - LLM provider  
- If needed, an â€œOpenAI API Keyâ€ password field is enabled.
- The key is kept in `session_state["openai_key"]`, never written to prefs.îˆ€citeîˆ‚turn1file0îˆ  

---

### 2.5 Phase 5 â€“ Chat & Results

The core RAG workflow.

#### Query handling & embedder selection

- Uses the active `persist_dir` and `vs_collection` (or falls back to prefs / new collection name).  
- Ensures the vector collection is opened via `open_vector_in_session`.  
- For embeddings at query time:
  - Tries to read `<collection>__meta.json` (provider + model)  
  - If available, this overrides the current UI selection to ensure consistency  
  - If not, falls back to the embedding provider/model chosen in the UIîˆ€citeîˆ‚turn1file0îˆ  
- Shows an info message if there is a mismatch between the embedding model used at index time and the one used at query time.

#### Retrieval from KB (tickets)

- Computes query embedding and runs `collection.query()` with `n_results = top_k`.  
- Filters results with `distance <= max_distance`.  
- Debug info shows:
  - raw number of results  
  - collection count  
  - first distances and threshold  

#### Retrieval from MEM (playbooks)

- If `enable_memory` is active:
  - Queries the separate `memories` collection  
  - Filters by distance threshold and TTL:
    - Only entries with `expires_at >= now` are kept  
  - Uses a cap `mem_cap = 2` to limit how many MEM items are blended.îˆ€citeîˆ‚turn1file0îˆ  

#### Blending KB + MEM and collapse logic

- MEM results (up to 2) are added first, then KB results until `top_k` total.  
- The combined list is processed twice via `collapse_by_parent`:
  - **View list**: `per_parent_display`, `stitch_for_prompt=False`  
  - **Prompt context**: `per_parent_prompt`, `stitch_for_prompt=True`, `stitch_max_chars` limit  
- Each group is built around `parent_id` / `id_readable` and sorted by distance and token position.îˆ€citeîˆ‚turn1file0îˆ  

#### Prompt and LLM answer

- System prompt (`RAG_SYSTEM_PROMPT`) instructs the model to:
  - Answer based on similar YouTrack tickets  
  - Always cite ticket IDs in brackets  
  - Ask for clarifications when context is insufficient  
  - Answer in **English**  
- The user prompt lists:
  - The new ticket text  
  - A summary of similar tickets with ID, distance, summary and first 500 characters  
- Optional â€œShow promptâ€ debug toggle displays the final prompt in an expander.îˆ€citeîˆ‚turn1file0îˆ  
- The answer is generated via `LLMBackend` using:
  - OpenAI Responses API (with fallback to Chat Completions)  
  - Or Ollama `/api/chat` with `stream=False` and robust JSON parsing fallback.îˆ€citeîˆ‚turn1file0îˆ  

#### Results display

- The final answer is shown at the top.  
- Below, a â€œSimilar results (topâ€‘k, with provenance)â€ section lists:
  - KB results:
    - Ticket ID + summary as a link back to YouTrack (when base URL is known)  
    - Optional distance and chunk information (ID, token offset)  
    - Chunk text in an expander  
  - MEM results:
    - Marked as `ğŸ§  Playbook` with title (if present)  
    - Optional distance  
    - Optional full text if `mem_show_full` is enabledîˆ€citeîˆ‚turn1file0îˆ  

---

### 2.6 Phase 6 â€“ Solutions Memory

This page manages the **playbook memory** stored in the separate `memories` collection.

- Global toggle `enable_memory`:
  - Controls whether the Chat phase can save and retrieve playbooks  
- `mem_ttl_days`: default TTL (days) applied to new playbooks  
- `mem_show_full`: controls whether full playbook text is shown in Chat results  
- `show_memories`: enables the table of saved playbooks on this pageîˆ€citeîˆ‚turn1file0îˆ  

**Delete all memories**

- â€œDelete all memoriesâ€ button:
  - Requires confirmation checkbox  
  - Deletes the `memories` collection and recreates it empty  

**Playbook table**

- When `show_memories` is enabled:
  - Reads all entries from `memories`  
  - Shows a dataframe with columns:
    - `ID`, `Project`, `Tags`, `Created`, `Expires`, `Preview` (short snippet)îˆ€citeîˆ‚turn1file0îˆ  

---

### 2.7 Phase 7 â€“ Preferences & Debug

- Toggle **Enable preferences memory (local)**:
  - If enabled, nonâ€‘sensitive prefs are stored in `.app_prefs.json` (local or `/tmp` in cloud).  
- â€œSave preferencesâ€:
  - Normalizes provider/model (e.g., forces OpenAI if Ollama is not available)  
  - Writes all relevant fields:
    - YouTrack URL  
    - persist_dir, collection names  
    - embedding backend/model  
    - LLM provider/model/temperature  
    - distance, chunking, advanced retrieval settings  
    - memory settings (TTL, show flags)  
- â€œRestore defaultsâ€:
  - Deletes the prefs file and reruns Streamlit.  

**Debug**

- â€œShow LLM promptâ€ checkbox: same flag used by the Chat phase to optionally display the prompt.îˆ€citeîˆ‚turn1file0îˆ  

---

## 3. Playbook Creation (Mark as Solved)

From the Chat page:

- If `enable_memory` is True and a last answer exists, you can press:  
  **â€œâœ… Mark as solved â†’ Save as playbookâ€**  
- The app:
  1. Builds a condensation prompt instructing the LLM to produce 3â€“6 imperative steps.  
  2. Calls the LLM (slightly lower temperature) to generate a compact playbook; on error, falls back to truncating the answer.  
  3. Builds metadata:
     - `source="memory"`, `project`, `quality="verified"`  
     - `created_at`, `expires_at = now + mem_ttl_days`  
     - `tags` including `playbook` and current project (if known)  
  4. Uses the current embedder to embed the playbook text and add it to `memories`.  
  5. Shows a caption with path, collection and count, and reopens the Solutions Memory page after rerun.îˆ€citeîˆ‚turn1file0îˆ  

---

## 4. Sidebar Wizard & Status Panels

The sidebar provides:

- Phase navigation (radio with 7 phases + progress bar)  
- YouTrack status (connected / not connected, current URL)  
- Vector DB / Embeddings summary:
  - persist_dir, active collection, embedding provider/model  
- LLM status:
  - provider, model, temperature  
- Retrieval summary (readâ€‘only):
  - Topâ€‘K, max distance, collapse duplicates  
  - Perâ€‘ticket aggregation and stitch limit  
  - Chunking settings (enabled, size, overlap, min size)  
  - Embeddings + collection summaryîˆ€citeîˆ‚turn1file0îˆ  
- Embedding status:
  - â€œIndexed withâ€ vs â€œQuery usingâ€ (provider + model + metadata source)  
  - Warning if there is a mismatch between indexed and query settings  

On nonâ€‘cloud environments, a **Quit** button closes the app (`os._exit(0)`).îˆ€citeîˆ‚turn1file0îˆ  

The sidebar also automatically opens the active collection (if any) and shows the number of indexed documents.îˆ€citeîˆ‚turn1file0îˆ  

---

## 5. Requirements & Installation

### 5.1 Python dependencies

Install from `requirements.txt`, typically including:

- `streamlit`  
- `chromadb`  
- `sentence-transformers` (for local embeddings)  
- `openai`  
- `tiktoken` (optional, for tokenâ€‘based chunking)  
- `requests`, `pandas` and other standard utilitiesîˆ€citeîˆ‚turn1file0îˆ  

```bash
pip install -r requirements.txt
```

### 5.2 Environment variables

Optional environment variables:

- `OPENAI_API_KEY` or `OPENAI_API_KEY_EXPERIMENTS`  
- `CHROMA_DIR` â€“ overrides default Chroma path  
- `OLLAMA_HOST` â€“ host/port for Ollama (default `http://localhost:11434`)îˆ€citeîˆ‚turn1file0îˆ  

---

## 6. Running the App

### 6.1 Streamlit mode (recommended)

```bash
streamlit run app.py --server.port 8502
```

Then open the browser at the URL printed by Streamlit.

### 6.2 CLI selfâ€‘tests

If Streamlit is not available and you run:

```bash
python app.py
```

the app prints basic usage help and runs minimal selfâ€‘tests:

- VectorStore initialization  
- Local embeddings (if `sentence-transformers` is installed)  
- LLM backend initialization for OpenAI / Ollama (when possible)îˆ€citeîˆ‚turn1file0îˆ  

---

## 7. Docker Notes

The app is Dockerâ€‘friendly but does not enforce any specific volume layout.  
A practical pattern is:

```text
project-root/
    app.py
    data/          â† local Chroma (when running on host)
    data_docker/   â† Chroma used inside Docker
```

Example `docker-compose.yml`:

```yaml
services:
  rag-support-app:
    build: .
    container_name: rag_support_app
    ports:
      - "8503:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      - STREAMLIT_SERVER_PORT=8501
    volumes:
      - ./data_docker:/app/data
      - ./.streamlit:/app/.streamlit:ro
    restart: unless-stopped
```

With this configuration:

- `APP_DIR` inside the container is `/app`  
- Default Chroma path becomes `/app/data/chroma`  
- Data is persisted under `./data_docker` on the host, separate from any local `./data`.îˆ€citeîˆ‚turn1file0îˆ  

If you get schema errors (e.g. from older local DBs), just remove `data_docker/chroma` and reindex.

---

## 8. License

See the `LICENSE` file if present in the repository.  
